{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9ddf8e-dd39-4485-aaa6-f07d7880ff55",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "---\n",
    "This file is to explore the Fashion-MNIST dataset for use in a deep learning classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9170315b-fbfc-443e-bac7-bc1e9f79e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfc696eb-16d8-49eb-aae1-b2750d7d607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device config\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9590899-0203-4cb7-a424-3f816606c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "num_epochs = 10\n",
    "batch_size = 4\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce4693b2-3530-40c6-bd27-b73934099706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset has PILImage images of range [0,1]\n",
    "# we transform them to tensors of normalized range [-1,1]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "# Fashion-MNIST: 28x28 images, 60,000 training examples and 10,000 testing samples \n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train = True,\n",
    "                                        download=False, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train = False,\n",
    "                                        download=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9ef110a-ac43-464b-8ca7-99c557a049c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
    "           'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e679f873-1c3d-4801-bd4e-73aa6b9e648b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bf58ded-1ba2-4787-bef6-14ec4b1c573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the cnn\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(x.shape)\n",
    "        x = self.fc3(x)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beb99aa6-0037-44a7-9b2c-01618f91ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b0156c9-d249-4ed4-bd61-8979ec34ee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/15000], Loss: 1.7031\n",
      "Epoch [1/10], Step [1001/15000], Loss: 0.4112\n",
      "Epoch [1/10], Step [2001/15000], Loss: 1.3499\n",
      "Epoch [1/10], Step [3001/15000], Loss: 1.2024\n",
      "Epoch [1/10], Step [4001/15000], Loss: 0.7449\n",
      "Epoch [1/10], Step [5001/15000], Loss: 0.9258\n",
      "Epoch [1/10], Step [6001/15000], Loss: 0.8829\n",
      "Epoch [1/10], Step [7001/15000], Loss: 0.2687\n",
      "Epoch [1/10], Step [8001/15000], Loss: 0.6588\n",
      "Epoch [1/10], Step [9001/15000], Loss: 0.0603\n",
      "Epoch [1/10], Step [10001/15000], Loss: 0.9402\n",
      "Epoch [1/10], Step [11001/15000], Loss: 0.5765\n",
      "Epoch [1/10], Step [12001/15000], Loss: 0.2848\n",
      "Epoch [1/10], Step [13001/15000], Loss: 0.2343\n",
      "Epoch [1/10], Step [14001/15000], Loss: 0.7964\n",
      "Epoch [2/10], Step [1/15000], Loss: 0.2432\n",
      "Epoch [2/10], Step [1001/15000], Loss: 0.9130\n",
      "Epoch [2/10], Step [2001/15000], Loss: 0.6167\n",
      "Epoch [2/10], Step [3001/15000], Loss: 0.4934\n",
      "Epoch [2/10], Step [4001/15000], Loss: 0.4110\n",
      "Epoch [2/10], Step [5001/15000], Loss: 0.7225\n",
      "Epoch [2/10], Step [6001/15000], Loss: 0.0494\n",
      "Epoch [2/10], Step [7001/15000], Loss: 0.8162\n",
      "Epoch [2/10], Step [8001/15000], Loss: 0.2927\n",
      "Epoch [2/10], Step [9001/15000], Loss: 0.2752\n",
      "Epoch [2/10], Step [10001/15000], Loss: 0.2413\n",
      "Epoch [2/10], Step [11001/15000], Loss: 1.4030\n",
      "Epoch [2/10], Step [12001/15000], Loss: 0.2166\n",
      "Epoch [2/10], Step [13001/15000], Loss: 0.4462\n",
      "Epoch [2/10], Step [14001/15000], Loss: 0.2801\n",
      "Epoch [3/10], Step [1/15000], Loss: 0.5212\n",
      "Epoch [3/10], Step [1001/15000], Loss: 0.0690\n",
      "Epoch [3/10], Step [2001/15000], Loss: 0.3763\n",
      "Epoch [3/10], Step [3001/15000], Loss: 0.0226\n",
      "Epoch [3/10], Step [4001/15000], Loss: 0.6561\n",
      "Epoch [3/10], Step [5001/15000], Loss: 1.9032\n",
      "Epoch [3/10], Step [6001/15000], Loss: 0.2550\n",
      "Epoch [3/10], Step [7001/15000], Loss: 0.7904\n",
      "Epoch [3/10], Step [8001/15000], Loss: 0.4602\n",
      "Epoch [3/10], Step [9001/15000], Loss: 0.6511\n",
      "Epoch [3/10], Step [10001/15000], Loss: 0.1042\n",
      "Epoch [3/10], Step [11001/15000], Loss: 0.1712\n",
      "Epoch [3/10], Step [12001/15000], Loss: 1.1180\n",
      "Epoch [3/10], Step [13001/15000], Loss: 0.6327\n",
      "Epoch [3/10], Step [14001/15000], Loss: 0.3431\n",
      "Epoch [4/10], Step [1/15000], Loss: 1.2788\n",
      "Epoch [4/10], Step [1001/15000], Loss: 0.1990\n",
      "Epoch [4/10], Step [2001/15000], Loss: 0.4368\n",
      "Epoch [4/10], Step [3001/15000], Loss: 0.2725\n",
      "Epoch [4/10], Step [4001/15000], Loss: 0.4002\n",
      "Epoch [4/10], Step [5001/15000], Loss: 0.4211\n",
      "Epoch [4/10], Step [6001/15000], Loss: 0.1882\n",
      "Epoch [4/10], Step [7001/15000], Loss: 0.8320\n",
      "Epoch [4/10], Step [8001/15000], Loss: 0.2635\n",
      "Epoch [4/10], Step [9001/15000], Loss: 0.0927\n",
      "Epoch [4/10], Step [10001/15000], Loss: 1.2751\n",
      "Epoch [4/10], Step [11001/15000], Loss: 0.1078\n",
      "Epoch [4/10], Step [12001/15000], Loss: 0.4100\n",
      "Epoch [4/10], Step [13001/15000], Loss: 0.2810\n",
      "Epoch [4/10], Step [14001/15000], Loss: 1.1486\n",
      "Epoch [5/10], Step [1/15000], Loss: 0.0811\n",
      "Epoch [5/10], Step [1001/15000], Loss: 0.1615\n",
      "Epoch [5/10], Step [2001/15000], Loss: 0.7017\n",
      "Epoch [5/10], Step [3001/15000], Loss: 0.3263\n",
      "Epoch [5/10], Step [4001/15000], Loss: 1.3982\n",
      "Epoch [5/10], Step [5001/15000], Loss: 0.2023\n",
      "Epoch [5/10], Step [6001/15000], Loss: 0.7538\n",
      "Epoch [5/10], Step [7001/15000], Loss: 0.8472\n",
      "Epoch [5/10], Step [8001/15000], Loss: 0.2537\n",
      "Epoch [5/10], Step [9001/15000], Loss: 0.6571\n",
      "Epoch [5/10], Step [10001/15000], Loss: 0.2362\n",
      "Epoch [5/10], Step [11001/15000], Loss: 1.0293\n",
      "Epoch [5/10], Step [12001/15000], Loss: 0.1113\n",
      "Epoch [5/10], Step [13001/15000], Loss: 0.1740\n",
      "Epoch [5/10], Step [14001/15000], Loss: 0.0210\n",
      "Epoch [6/10], Step [1/15000], Loss: 0.1457\n",
      "Epoch [6/10], Step [1001/15000], Loss: 0.4731\n",
      "Epoch [6/10], Step [2001/15000], Loss: 1.8609\n",
      "Epoch [6/10], Step [3001/15000], Loss: 0.5444\n",
      "Epoch [6/10], Step [4001/15000], Loss: 0.0490\n",
      "Epoch [6/10], Step [5001/15000], Loss: 0.1753\n",
      "Epoch [6/10], Step [6001/15000], Loss: 0.0817\n",
      "Epoch [6/10], Step [7001/15000], Loss: 0.1234\n",
      "Epoch [6/10], Step [8001/15000], Loss: 0.8761\n",
      "Epoch [6/10], Step [9001/15000], Loss: 1.0181\n",
      "Epoch [6/10], Step [10001/15000], Loss: 0.1306\n",
      "Epoch [6/10], Step [11001/15000], Loss: 0.8623\n",
      "Epoch [6/10], Step [12001/15000], Loss: 0.1125\n",
      "Epoch [6/10], Step [13001/15000], Loss: 0.2694\n",
      "Epoch [6/10], Step [14001/15000], Loss: 0.4351\n",
      "Epoch [7/10], Step [1/15000], Loss: 0.6215\n",
      "Epoch [7/10], Step [1001/15000], Loss: 1.5418\n",
      "Epoch [7/10], Step [2001/15000], Loss: 0.1334\n",
      "Epoch [7/10], Step [3001/15000], Loss: 0.0068\n",
      "Epoch [7/10], Step [4001/15000], Loss: 0.2187\n",
      "Epoch [7/10], Step [5001/15000], Loss: 0.5405\n",
      "Epoch [7/10], Step [6001/15000], Loss: 0.2158\n",
      "Epoch [7/10], Step [7001/15000], Loss: 0.4059\n",
      "Epoch [7/10], Step [8001/15000], Loss: 0.1587\n",
      "Epoch [7/10], Step [9001/15000], Loss: 0.2274\n",
      "Epoch [7/10], Step [10001/15000], Loss: 0.1058\n",
      "Epoch [7/10], Step [11001/15000], Loss: 1.3957\n",
      "Epoch [7/10], Step [12001/15000], Loss: 0.1278\n",
      "Epoch [7/10], Step [13001/15000], Loss: 0.2142\n",
      "Epoch [7/10], Step [14001/15000], Loss: 0.8397\n",
      "Epoch [8/10], Step [1/15000], Loss: 0.2040\n",
      "Epoch [8/10], Step [1001/15000], Loss: 0.6230\n",
      "Epoch [8/10], Step [2001/15000], Loss: 0.3291\n",
      "Epoch [8/10], Step [3001/15000], Loss: 0.0235\n",
      "Epoch [8/10], Step [4001/15000], Loss: 0.1253\n",
      "Epoch [8/10], Step [5001/15000], Loss: 0.8122\n",
      "Epoch [8/10], Step [6001/15000], Loss: 0.5946\n",
      "Epoch [8/10], Step [7001/15000], Loss: 0.0122\n",
      "Epoch [8/10], Step [8001/15000], Loss: 0.4648\n",
      "Epoch [8/10], Step [9001/15000], Loss: 0.1669\n",
      "Epoch [8/10], Step [10001/15000], Loss: 0.0522\n",
      "Epoch [8/10], Step [11001/15000], Loss: 0.2076\n",
      "Epoch [8/10], Step [12001/15000], Loss: 0.0193\n",
      "Epoch [8/10], Step [13001/15000], Loss: 0.2042\n",
      "Epoch [8/10], Step [14001/15000], Loss: 2.2164\n",
      "Epoch [9/10], Step [1/15000], Loss: 0.0065\n",
      "Epoch [9/10], Step [1001/15000], Loss: 0.0362\n",
      "Epoch [9/10], Step [2001/15000], Loss: 0.1103\n",
      "Epoch [9/10], Step [3001/15000], Loss: 0.0070\n",
      "Epoch [9/10], Step [4001/15000], Loss: 0.2885\n",
      "Epoch [9/10], Step [5001/15000], Loss: 0.0278\n",
      "Epoch [9/10], Step [6001/15000], Loss: 0.1726\n",
      "Epoch [9/10], Step [7001/15000], Loss: 0.2840\n",
      "Epoch [9/10], Step [8001/15000], Loss: 0.6042\n",
      "Epoch [9/10], Step [9001/15000], Loss: 0.0225\n",
      "Epoch [9/10], Step [10001/15000], Loss: 0.0781\n",
      "Epoch [9/10], Step [11001/15000], Loss: 0.0518\n",
      "Epoch [9/10], Step [12001/15000], Loss: 0.0495\n",
      "Epoch [9/10], Step [13001/15000], Loss: 0.1724\n",
      "Epoch [9/10], Step [14001/15000], Loss: 0.1215\n",
      "Epoch [10/10], Step [1/15000], Loss: 0.2102\n",
      "Epoch [10/10], Step [1001/15000], Loss: 0.1151\n",
      "Epoch [10/10], Step [2001/15000], Loss: 0.0969\n",
      "Epoch [10/10], Step [3001/15000], Loss: 0.0557\n",
      "Epoch [10/10], Step [4001/15000], Loss: 0.0129\n",
      "Epoch [10/10], Step [5001/15000], Loss: 0.3738\n",
      "Epoch [10/10], Step [6001/15000], Loss: 0.4806\n",
      "Epoch [10/10], Step [7001/15000], Loss: 0.0659\n",
      "Epoch [10/10], Step [8001/15000], Loss: 0.4728\n",
      "Epoch [10/10], Step [9001/15000], Loss: 0.4181\n",
      "Epoch [10/10], Step [10001/15000], Loss: 0.0478\n",
      "Epoch [10/10], Step [11001/15000], Loss: 0.0777\n",
      "Epoch [10/10], Step [12001/15000], Loss: 1.1725\n",
      "Epoch [10/10], Step [13001/15000], Loss: 0.0349\n",
      "Epoch [10/10], Step [14001/15000], Loss: 0.1588\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i%1000) == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb029dde-6143-4fc3-8597-7c7a5d5b9aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n",
      "Accuracy of the network: 86.46 %\n",
      "Accuracy of T-shirt/top: 77.8 %\n",
      "Accuracy of Trouser: 96.5 %\n",
      "Accuracy of Pullover: 73.9 %\n",
      "Accuracy of Dress: 89.0 %\n",
      "Accuracy of Coat: 76.7 %\n",
      "Accuracy of Sandal: 91.5 %\n",
      "Accuracy of Shirt: 71.2 %\n",
      "Accuracy of Sneaker: 96.8 %\n",
      "Accuracy of Bag: 96.2 %\n",
      "Accuracy of Ankle boot: 95.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_kernel",
   "language": "python",
   "name": "pytorch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
